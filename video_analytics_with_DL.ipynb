{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video_analytics_with_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjf0A1F+1r0GqSKc8Jl8rI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmouts/PPS_MultiComms/blob/master/video_analytics_with_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ5QXlCZ_Ify"
      },
      "source": [
        "# Ανάλυση βίντεο με τεχνικές Βαθιάς Μάθησης"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsXsxzpy-vS3"
      },
      "source": [
        "## Εισαγωγή\r\n",
        "Σε αυτό το εργαστήριο θα χρησιμοποιήσουμε τεχνικές Deep Learning για να αναγνωρίσουμε διαφορετικούς χαρακτήρες σε ένα βίντεο, και το χρόνο που παρουσιάζεται κάθε ένας. Θα δουλέψουμε με Python σε ένα απλό βίντεο κινούμενων σχεδίων! (Tom & Jerry).\r\n",
        "\r\n",
        "Τα βήματα που θα ακολουθήσουμε είναι τα εξής:\r\n",
        "\r\n",
        "1.   Διάβασμα βίντεο και εξαγωγή  σκηνών (frames)\r\n",
        "2.   Εξαγωγή χαρακτηριστικών - Εκπαίδευση μοντέλου\r\n",
        "3.   Υπολογισμός σκηνικού χρόνου - Μια απλή λύση\r\n",
        "4.   Σκέψεις για βελτίωση\r\n",
        "\r\n",
        "## 1.   Διάβασμα βίντεο και εξαγωγή  σκηνών (frames)\r\n",
        "Το βίντεο δεν είναι τίποτε παραπάνω από μια συλλογή εικόνων. Αυτές οι εικόνες ονομάζονται σκηνές (frames) και μπορούν να συνδιαστούν για να πάρουμε το αρχικό βίντεο. Ετσι, το πρόβλημα σε σχέση με δεδομένα βίντεο δεν είναι τόσο διαφορετικό απο την ταξινόμηση εικόνας ή την αναγνώριση αντικειμένων. Απλώς υπάρχει ένα επιπλέον βήμα του τα εξάγουμε σκηνές από το βίντεο.\r\n",
        "\r\n",
        "Μιάς και το πρόβλημά μας είναι να υπολογίσουμε το χρόνο κάθε χαρακτήρα στο βίντεο, ας δούμε πιο αναλυτικά τα βήματα που θα ακολουθήσουμε:\r\n",
        "\r\n",
        "\r\n",
        "*   Εισαγωγή και ανάγνωση του βίντεο, εξαγωγή σκηνών και αποθήκευση ως εικόνες.\r\n",
        "*   Χαρακτηρισμός κάποιων εικόνων για εκπαίδευση του μοντέλου.\r\n",
        "*   Δημιουργία μοντέλου με εκπαίδευση.\r\n",
        "*   Προβλέψεις για νέες εικόνες.\r\n",
        "*   Υπολογισμός σκηνικού χρόνου.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av0UjgXhDbWS"
      },
      "source": [
        "##Χειρισμός αρχείων βίντεο με Python\r\n",
        "Ξεκινάμε με την εισαγωγή απαραίτητων βιβλιοθηκών:\r\n",
        "\r\n",
        "    Numpy\r\n",
        "    Pandas\r\n",
        "    Matplotlib\r\n",
        "    Keras\r\n",
        "    Skimage\r\n",
        "    OpenCV\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jRH6LQgECe4"
      },
      "source": [
        "!apt install ffmpeg\r\n",
        "import cv2     # for capturing videos\r\n",
        "import math   # for mathematical operations\r\n",
        "import matplotlib.pyplot as plt    # for plotting the images\r\n",
        "%matplotlib inline\r\n",
        "import pandas as pd\r\n",
        "from keras.preprocessing import image   # for preprocessing the images\r\n",
        "import numpy as np    # for mathematical operations\r\n",
        "from keras.utils import np_utils\r\n",
        "from skimage.transform import resize   # for resizing images\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDRw8RgtGeRD"
      },
      "source": [
        "### Βήμα – 1: Διάβασμα βίντεο, εξαγωγή σκηνών και αποθήκευση ως εικόνες\r\n",
        "\r\n",
        "Ας κατεβάσουμε το βίντεο με την συνάρτηση `wget`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9-cFoHBHOp8"
      },
      "source": [
        "#!pip install gdown\r\n",
        "import gdown\r\n",
        "\r\n",
        "url = 'https://github.com/kmouts/PPS_MultiComms/blob/master/Tom_jerry.mp4?raw=true'\r\n",
        "output = 'Tom_jerry.mp4'\r\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azf20npOKzkP"
      },
      "source": [
        "!ls -lah"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49OOThAoKxp_"
      },
      "source": [
        "Θα φορτώσουμε το βίντεο με τη συνάρτηση VideoCapture() και θα το μετατρέψουμε σε σκηνές (εικόνες) που θα τις σώσουμε με την συνάρτηση imwrite()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbBDFKRUMvZl"
      },
      "source": [
        "import os\r\n",
        "if not os.path.isdir( 'tom' ) :\r\n",
        "    os.mkdir( 'tom' )  # make sure the directory exists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEKlxpA9LBYp"
      },
      "source": [
        "count = 0\r\n",
        "videoFile = \"Tom_jerry.mp4\"\r\n",
        "cap = cv2.VideoCapture(videoFile)   # capturing the video from the given path\r\n",
        "frameRate = cap.get(5) #frame rate\r\n",
        "x=1\r\n",
        "while(cap.isOpened()):\r\n",
        "    frameId = cap.get(1) #current frame number\r\n",
        "    ret, frame = cap.read()\r\n",
        "    if (ret != True):\r\n",
        "        break\r\n",
        "    if (frameId % math.floor(frameRate) == 0):\r\n",
        "        filename =\"/content/tom/frame%d.jpg\" % count;count+=1\r\n",
        "        if not cv2.imwrite(filename, frame):\r\n",
        "          raise Exception(\"Could not write image\")\r\n",
        "cap.release()\r\n",
        "print (\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq-nD9CDMNtB"
      },
      "source": [
        "# !ls -la tom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SBKy8MLLZPl"
      },
      "source": [
        "Οι εικόνες τώρα δημιουργήθηκαν. Ας δούμε μια εικόνα (σκηνή). Θα τη διαβάσουμε με την συνάρτηση `imread()` \r\n",
        "\r\n",
        "Let us try to visualize an image (frame). We will first read the image using the imread() function of matplotlib, and then plot it using the imshow() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8DMKzvrLr2I"
      },
      "source": [
        "img = plt.imread('tom/frame0.jpg')   # reading image using its name\r\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQm2_3D1aRb4"
      },
      "source": [
        "Αυτή είναι η πρώτη σκηνή από το βίντεο. Εξάγαμε μία σκηνή ανά δευτερόλεπτο, για όλη τη διάρκεια του βίντεο. Αφού η διάρκεια είναι 4:58 λεπτά (298 δευτερόλεπτα), έχουμε εξάγει συνολικά 298 εικόνες. \r\n",
        "\r\n",
        "Το πρόβλημά μας είναι να ταυτοποιήσουμε ποιές εικόνες έχουν τον Tom και ποιές τον Jerry. Αν οι εικόνες μας ήταν όμοιες με αυτές του Imagenet dataset, η διαδικασία διαχωρισμού θα ήταν εύκολη. Θα χρησιμοποιούσαμε κάποιο προ-εκπαιδευμένο μοντέλο στο Imagelnet και θα είχαμε και μεγάλο ποσοστό ακρίβειας! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxhVAJDUcApx"
      },
      "source": [
        "### Βήμα - 2: Χαρακτηρισμός κάποιων εικόνων για εκπαίδευση\r\n",
        "Πως θα προχωρήσουμε; Μια λύση είναι να δώσουμε χειροκίνητα χαρακτηρισμούς σε μερικές από τις εικόνες μας, και με αυτές να εκπαιδεύσουμε ένα μοντέλο. Στη συνέχεια θα χρησιμοποιήσουμε αυτό το μοντέλο για να κάνουμε πρόβλεψη των υπόλοιπων εικόνων που δεν έχουν χρησιμοποιηθεί στην εκπαίδευση.\r\n",
        "\r\n",
        "Ας σημειωθεί ότι μπορεί να υπάρχουν σκηνές που δεν υπάρχει κανείς από τους 2 χαρακτήρες. Έχουμε δηλαδή ένα πρόβλημα ταξινόμησης με πολλαπλές κλάσεις. Οι κλάσεις που έχουμε ορίσει εδώ είναι:\r\n",
        "\r\n",
        "*  0 – ούτε JERRY ούτε TOM\r\n",
        "*  1 – JERRY\r\n",
        "*  2 – TOM\r\n",
        "\r\n",
        "Ας διαβάσουμε το αρχείο mapping.csv με τους χαρακτηρισμούς:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5L4C9VfdhWH"
      },
      "source": [
        "import pandas as pd\r\n",
        "url_to_the_file  = \"https://raw.githubusercontent.com/kmouts/PPS_MultiComms/master/mapping.csv\"\r\n",
        "data = pd.read_csv(url_to_the_file, sep=',')\r\n",
        "data.head()      # printing first five rows of the file\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K72TkJbksbF"
      },
      "source": [
        "Το αρχείο αντιστοίχισης έχει δύο στήλες: \r\n",
        "\r\n",
        "**Image_ID**: Το όνομα κάθε εικόνας\r\n",
        "**Class**: Η κλάση\r\n",
        "Το επόμενο βήμα είναι να διαβάσουμε τις εικόνες με βάση το όνομά τους (δηλ. η στήλη Image_ID column)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3kT4FMce8lE"
      },
      "source": [
        "X = [ ]     # creating an empty array\r\n",
        "for img_name in data.Image_ID:\r\n",
        "    img = plt.imread('tom/' + img_name)\r\n",
        "    X.append(img)  # storing each image in array X\r\n",
        "X = np.array(X)    # converting list to array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqQquwlrlzhF"
      },
      "source": [
        "Έχουμε τώρα τις εικόνες σε μια λίστα. Θυμόμαστε ότι χρειαζόμαστε δύο πράγματα για να εκπαιδεύσουμε το μοντέλο μας:\r\n",
        "*  Τις εικόνες εκπαίδευσης\r\n",
        "*  Τις αντίστοιχες κλάσεις τους\r\n",
        "\r\n",
        "Εφόσον έχουμε τρεις κλάσεις, θα τις κωδικοποιήσουμε (hot encode) με τη συνάρτηση `to_categorical()` από τη βιβλιοθήκη `keras.utils`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06oLKFKmmfUt"
      },
      "source": [
        "y = data.Class\r\n",
        "dummy_y = np_utils.to_categorical(y)    # one hot encoding Classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_UIhjfBmriL"
      },
      "source": [
        "Θα χρησιμοποιήσουμε ένα προεκπαιδευμένο VGG16 μοντέλο, που παίρνει σαν είσοδο εικόνες μεγέθους (224 X 224 X 3). Εφόσον οι εικόνες μας είναι διαφορετικού μεγέθους, θα τους αλλάξουμε μέγεθος με τη συνάρτηση `resize()` από τη βιβλιοθήκη `skimage.transform`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxStCjS9nRH6"
      },
      "source": [
        "image = []\r\n",
        "for i in range(0,X.shape[0]):\r\n",
        "    a = resize(X[i], preserve_range=True, output_shape=(224,224)).astype(int)      # reshaping to 224*224*3\r\n",
        "    image.append(a)\r\n",
        "X = np.array(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM8AzTIPng9p"
      },
      "source": [
        "'Ολες οι εικόνες μας τώρα έχουμε μέγεθος 224 X 224 X 3. Αλλά πριν τις δώσουμε στο μοντέλο ως είσοδο, πρέπει να τις προεπεξεργαστούμε ανάλογα με τις απαιτήσεις του μοντέλου, αλλιώς δεν θα έχουμε καλή απόδοση. Εδώ χρησιμοποιούμε τη συνάρτηση  [`preprocess_input()`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input) από τη βιβλιοθήκη `keras.applications.vgg16`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I4A5gWEo0e-"
      },
      "source": [
        "from keras.applications.vgg16 import preprocess_input\r\n",
        "X = preprocess_input(X)      # preprocessing the input data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLAmlQLio__7"
      },
      "source": [
        "Χρειαζόμαστε και ένα σύνολο εικόνων επιβεβαίωσης (validation) για να ελέγχουμε την απόδοση του μοντέλου σε εικόνες που δεν έχει δει. Θα κάνουμε χρήση της συνάρτησης `train_test_split()` από το module `sklearn.model_selection` για τον τυχαίο διαχωρισμό των εικόνων σε τμήμα εκπαίδευσης και επιβεβαίωσης."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsQ9Gw52pjMC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, dummy_y, test_size=0.3, random_state=42)    # preparing the validation set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXkkIK0BpohP"
      },
      "source": [
        "###Βήμα 3: Κατασκευάζοντας το μοντέλο\r\n",
        "Θα κατασκευάσουμε το μοντέλο χρησιμοποιώντας, όπως είπαμε, ένα προεκπαιδευμένο  VGG16. Ας εισάγουμε τις απαραίτητες **βιβλιοθήκες**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SU__LK_qCHJ"
      },
      "source": [
        "from keras.models import Sequential\r\n",
        "from keras.applications.vgg16 import VGG16\r\n",
        "from keras.layers import Dense, InputLayer, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0xV7kIXqIr5"
      },
      "source": [
        "Εισάγουμε τώρα το προεκπαιδευμένο μοντέλο VGG16 και το αποθηκεύουμε ως βασικό μοντέλο:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F7aFxa8qUbB"
      },
      "source": [
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))    # include_top=False to remove the top layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nH1AVndqaJo"
      },
      "source": [
        "Θα κάνουμε προβλέψεις με χρήση αυτού του μοντέλου για `X_train` και `X_valid`, θα εξάγουμε τα χαρακτηριστικά, και στη συνέχεια θα χρησιμοποιήσουμε αυτά τα χαρακτηριστικά για να επανεκπαιδεύσουμε το μοντέλο."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5t0sM1irBPR"
      },
      "source": [
        "X_train = base_model.predict(X_train)\r\n",
        "X_valid = base_model.predict(X_valid)\r\n",
        "X_train.shape, X_valid.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JofAm8MrKRG"
      },
      "source": [
        "Η μορφή των `X_train` και `X_valid` είναι (208, 7, 7, 512), (90, 7, 7, 512) αντίστοιχα. Για να τα περάσουμε στο νευρωνικό μας δίκτυο θα πρέπει να τα αναμορφώσουμε σε 1-D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyWq-SHYrlJC"
      },
      "source": [
        "X_train = X_train.reshape(208, 7*7*512)      # converting to 1-D\r\n",
        "X_valid = X_valid.reshape(90, 7*7*512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqBV0g1erq6u"
      },
      "source": [
        "Τώρα θα προεπεξεργαστούμε τις εικόνες και θα τις κάνουμε να είναι επικεντρωμένες στο μηδέν (zero-centered) που βοηθά το μοντέλο να συγκλίνει γρηγορότερα."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVS8KFMLsHgz"
      },
      "source": [
        "train = X_train/X_train.max()      # centering the data\r\n",
        "X_valid = X_valid/X_train.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekZuvIhjsd0o"
      },
      "source": [
        "Τελικά, θα κατασκευάσουμε το μοντέλο μας. Αυτό χωρίζεται σε τρία βήματα:\r\n",
        "\r\n",
        "1. Κατασκευή μοντέλου\r\n",
        "2. Μεταγλώττιση (compiling) μοντέλου\r\n",
        "3. Εκπαίδευση μοντέλου"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw7xzVPStGGF"
      },
      "source": [
        "# i. Building the model\r\n",
        "model = Sequential()\r\n",
        "model.add(InputLayer((7*7*512,)))    # input layer\r\n",
        "model.add(Dense(units=1024, activation='sigmoid')) # hidden layer\r\n",
        "model.add(Dense(3, activation='softmax'))    # output layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzULtTKctdub"
      },
      "source": [
        "Ας δούμε τη σύνοψη του μοντέλου με χρήση της συνάρτησης `summary()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4Ac2EQWtQaT"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec4Ar78ntqFF"
      },
      "source": [
        "Έχουμε ένα κρυφό επίπεδο (hidden) με 1,024 νευρώνες και ένα επίπεδο εξόδου με 3 νευρώνες (αφού έχουμε 3 κλάσεις να προβλέψουμε). Ας μεταγλωττίσουμε το μοντέλο:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTEgeZMNuC9g"
      },
      "source": [
        "# ii. Compiling the model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9kAjq2yuGYE"
      },
      "source": [
        "Στο τελικό βήμα, θα εκπαιδεύσουμε το μοντέλο και παράλληλα θα ελέγουμε την απόδοσή του σε εικόνες που δεν έχει δει (το σύνολο εικόνων επιβεβαίωσης):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQLtg-1AuWcg"
      },
      "source": [
        "# iii. Training the model\r\n",
        "model.fit(train, y_train, epochs=50, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAEG9HDbvJ3e"
      },
      "source": [
        "Μπορούμε να δούμε ότι η απόδοσή του είναι πολύ καλή στις εικόνες εκπαίδευσης και στις εικόνες επιβεβαίωσης. Έχουμε ακρίβεια πάνω από 86% σε εικόνες που δεν έχει δει. Και έτσι εκπαιδεύσαμε ένα μοντέλο σε δεδομένα από βίντεο για να παίρνουμε προβλέψεις για κάθε σκηνή.\r\n",
        "Στο επόμενο βήμα, θα προσπαθήσουμε να υπολογίσουμε τον σκηνικό χρόνο των TOM και JERRY σε ένα νέο βίντεο.\r\n",
        "\r\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul_HXruJwSQC"
      },
      "source": [
        "##3. Υπολογισμός σκηνικού χρόνου – Μια απλή λύση\r\n",
        "Θα κατεβάσουμε το νέο βίντεο όπως και πρίν, και θα εξάγουμε τις σκηνές:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR7WivPRw43i"
      },
      "source": [
        "url = 'https://github.com/kmouts/PPS_MultiComms/blob/master/Tom_Jerry_3.mp4?raw=true'\r\n",
        "output = 'Tom_jerry_3.mp4'\r\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXKx4rKpxEUZ"
      },
      "source": [
        "import os\r\n",
        "if not os.path.isdir( 'tom3' ) :\r\n",
        "    os.mkdir( 'tom3' )  # make sure the directory exists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpAvilY-woC5"
      },
      "source": [
        "count = 0\r\n",
        "videoFile = \"Tom_jerry_3.mp4\"\r\n",
        "cap = cv2.VideoCapture(videoFile)\r\n",
        "frameRate = cap.get(5) #frame rate\r\n",
        "x=1\r\n",
        "while(cap.isOpened()):\r\n",
        "    frameId = cap.get(1) #current frame number\r\n",
        "    ret, frame = cap.read()\r\n",
        "    if (ret != True):\r\n",
        "        break\r\n",
        "    if (frameId % math.floor(frameRate) == 0):\r\n",
        "        filename =\"tom3/test%d.jpg\" % count;count+=1\r\n",
        "        cv2.imwrite(filename, frame)\r\n",
        "cap.release()\r\n",
        "print (\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_odAE38yNAD"
      },
      "source": [
        "# ls -lah tom3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Anpwt5l6zB2s"
      },
      "source": [
        "Μετά την εξαγωγή των σκηνών από το νέο βίντεο, θα φορτώσουμε το αρχείο `test.csv` που περιέχει τα ονόματα των εικόνων:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxgk3vlxza9o"
      },
      "source": [
        "url_to_the_file2  = \"https://raw.githubusercontent.com/kmouts/PPS_MultiComms/master/test.csv\"\r\n",
        "test = pd.read_csv(url_to_the_file2, sep=',')\r\n",
        "test.head()      # printing first five rows of the file\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NJ4MWvyzrM2"
      },
      "source": [
        "Στη συνέχεια, εισάγωουμε τις εικόνες προς δοκιμή και τις αλλάζουμε το μέγεθος σύμφωνα με τις απαιτήσεις του προεκπαιδευμένου μοντέλου:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DxeAFqsz8mL"
      },
      "source": [
        "test_image = []\r\n",
        "for img_name in test.Image_ID:\r\n",
        "    img = plt.imread('tom3/' + img_name)\r\n",
        "    test_image.append(img)\r\n",
        "test_img = np.array(test_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb1MUsxl0FES"
      },
      "source": [
        "test_image = []\r\n",
        "for i in range(0,test_img.shape[0]):\r\n",
        "    a = resize(test_img[i], preserve_range=True, output_shape=(224,224)).astype(int)\r\n",
        "    test_image.append(a)\r\n",
        "test_image = np.array(test_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JBQtesa0LMu"
      },
      "source": [
        "Θα χρειαστεί να κάνουμε και σε αυτές τις εικόνες τις αλλαγές που κάναμε και στις εικόνες εκπαίδευσης: θα τις προεπεξεργαστούμε, θα χρησιμοποιήσουμε τη συνάρτηση `base_model.predict()` για την εξαγωγή χαρακτηριστικών με το προεκπαιδευμένο μοντέλο VGG16, αλλαγή σε μορφή 1-D και zero-centered:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIM3URft0ugA"
      },
      "source": [
        "# preprocessing the images\r\n",
        "test_image = preprocess_input(test_image)\r\n",
        "\r\n",
        "# extracting features from the images using pretrained model\r\n",
        "test_image = base_model.predict(test_image)\r\n",
        "\r\n",
        "# converting the images to 1-D form\r\n",
        "test_image = test_image.reshape(186, 7*7*512)\r\n",
        "\r\n",
        "# zero centered images\r\n",
        "test_image = test_image/test_image.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv-TZDBC09h1"
      },
      "source": [
        "Είμαστε έτοιμοι να χρησιμοποιήσουμε το μοντέλο που εκπαιδεύσαμε προηγουμένως για την πρόβλεψη αυτών των εικόνων."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfEScvYr1Lqz"
      },
      "source": [
        "### Βήμα – 4: Προβλέψεις για τις νέες εικόνες"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BCJ0VuW1Y5X"
      },
      "source": [
        "# predictions = model.predict_classes(test_image) # deprecated\r\n",
        "predictions = np.argmax(model.predict(test_image), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ioqi36m2Ryx"
      },
      "source": [
        "### Βήμα – 5 Υπολογισμός σκηνικού χρόνου για TOM και JERRY\r\n",
        "Θυμόμαστε πως η κλάση ‘1’ αντιπροσωπεύει παρουσία του JERRY, και ‘2’ του TOM. Θα χρησιμοποιήσουμε αυτές τις προγνώσεις για να υπολογίσουμε τον σκηνικό χρόνο τους:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B1NOFB_2xbQ"
      },
      "source": [
        "print(\"The screen time of JERRY is\", predictions[predictions==1].shape[0], \"seconds\")\r\n",
        "print(\"The screen time of TOM is\", predictions[predictions==2].shape[0], \"seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17KH4KOA5wJJ"
      },
      "source": [
        "##3. Σκέψεις για βελτίωση...\r\n",
        "Πως θα μπορούσαμε να έχουμε καλύτερη απόδοση;\r\n",
        "\r\n",
        "*   Καλύτερο μοντέλο\r\n",
        "*   Βάρη στις κλάσεις\r\n",
        "*   Checkpoint best model\r\n",
        "*   Περισσότερες εικόνες εκπαίδευσης\r\n",
        "*   Multi-class, multi-label problem: object detection\r\n",
        "*   ...\r\n",
        "\r\n"
      ]
    }
  ]
}